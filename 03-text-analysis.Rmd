---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Анализ текста

```{r, message=FALSE}
library(tidyverse)
```

```{r, include=FALSE}
theme_set(theme_bw())
```


## Данные

Для работы мы воспользуемся двумя датасетами:

* Рассказы М. Зощенко

```{r,message=FALSE}
zo <- read_csv("https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/zoshenko.csv")
zo
```

* Курс начертательной геометрии под редакцией В.Гордона

```{r,message=FALSE}
geom <- read_csv("https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/gordon_geometry.csv")
```

Для начала лемматизируем полуичвшиеся тексты:

```{r, cache=TRUE}
library(udpipe)
rus <- udpipe_load_model("russian-syntagrus-ud-2.4-190531.udpipe")
geom_tokenized <- udpipe(geom, object = rus)
zo_tokenized <- udpipe(zo, object = rus)
```

Уберем стопслова и леммы, содержащие цифры и знаки препинания

```{r}
library(stopwords)
sw <- tibble(lemma = stopwords(language = "ru"))

geom_tokenized %>% 
  bind_rows(zo_tokenized) %>% 
  filter(!str_detect(lemma, "\\W|\\d")) %>% 
  anti_join(sw) %>% 
  select(doc_id, sentence_id, lemma) ->
  all_texts
all_texts
```

## tf-idf

tf-idf --- важная мера, которая позволяет выделять важные для текста слова.

$$tf = \frac{количество\ употреблений\ единицы\ в\ тексте}{количество\ уникальных\ единиц\ в тексте}$$
$$idf = log\left(\frac{количество\ документов\ в\ корпусе}{количество\ документов\ с\ исследуемой\ единицей}\right)$$
$$TfIdf = tf \times idf$$

```{r, fig.height=10, fig.width=14, message=FALSE}
library(tidytext)
all_texts %>% 
  count(doc_id, lemma) %>% 
  bind_tf_idf(lemma, doc_id, n) %>% 
  arrange(tf_idf) %>% 
  group_by(doc_id) %>% 
  top_n(5) %>% 
  ungroup() %>% 
  mutate(lemma = reorder_within(lemma, tf_idf, doc_id)) %>% 
  ggplot(aes(tf_idf, lemma))+
  geom_col()+
  facet_wrap(~doc_id, scales = "free")+
  scale_y_reordered()
```



## Подсказка слова

На прошлом занятии мы разобрались, что пакет `tidytext` позволяет делить не только на отдльные слова, но и смотреть на биграммы. Частотность биграмм можно использовать в подсказке слова, которую мы видим в наших телефонах:

```{r}
zo %>% 
  bind_rows(geom) %>% 
  unnest_tokens()
```


## Анализ тональности

* [Linis Crowd](http://linis-crowd.org/)
    * лемма
    * значение
    * среднеквадратичное отклонение
* [РуСентиЛекс](http://www.labinform.ru/pub/rusentilex/index.htm):
    * слово или словосочетание,
    * часть речи или синтаксический тип группы,
    * слово или словосочетание в лемматизированной форме, 
    * тональность: позитивная (positive), негативная(negative), нейтральная (neutral) или неопределеная оценка, зависит от контекста (positive/negative),
    * источник: оценка (opinion), чувство (feeling), факт (fact),
    * если тональность отличается для разных значений многозначного слова, то перечисляются все значения слова по тезаурусу РуТез и дается отсылка на сооветствующее понятие - имя понятия в кавычках.

Мы будем использовать [датасет](https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/ru_sentiment_linis-crowd.csv), составленный на базе Linis Crowd

```{r, fig.height=10, fig.width=14}
ru_sentiments <- read_csv("https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/ru_sentiment_linis-crowd.csv")

all_texts %>% 
  group_by(doc_id) %>% 
  left_join(ru_sentiments, by = c("lemma" = "words")) %>% 
  mutate(value = ifelse(is.na(value), 0, value)) %>% 
  group_by(doc_id, sentence_id) %>% 
  summarise(value = sum(value)) %>% 
  mutate(color = ifelse(value >= 0, "positive", "negative")) %>% 
  ggplot(aes(sentence_id, value, fill = color))+
  geom_col()+
  facet_wrap(~doc_id, scales = "free")
```


## Тематическое моделирование

```{r}
library(topicmodels)

all_texts %>% 
  count(doc_id, lemma, sort = TRUE) %>%   
  cast_dtm(doc_id, lemma, n) %>% 
  LDA(k = 2, control = list(seed = 42)) ->
  lda

lda %>% 
  tidy(matrix = "beta") %>% 
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% 
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

lda %>% 
  tidy(matrix = "beta") %>% 
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .006 | topic2 > .002) %>%
  mutate(log_ratio = log2(topic2 / topic1),
         term = fct_reorder(term, log_ratio)) %>% 
  ggplot(aes(log_ratio, term))+
  geom_col()

lda %>% 
  tidy(matrix = "gamma") 
```

